---
title: "Build your own async"
excerpt: "This article provides some information on how to build your own async in Scala"
category: guide
tags: [async, continuations, graalvm, scala, scala-3]
publishedDate: 2025-10-23
updatedDate: 2025-10-23
author: chia-hung-lin
repositoryUrl: https://codeberg.org/chlin501/async4s
---

## Introduction

When writing programmes with the aid of async/ await pattern code, have you ever wondered how [async](https://en.wikipedia.org/wiki/Asynchronous_I/O) works under the hood? I have a similar question. Here is the journey of exploring an apporach alternative to monadic operations style employeed by libraries such as ZIO, cats-effect.

## Concepts

Before starting, two concepts are important, including

1. [Coroutine](#coroutine)

2. [Event Loop](#event-loop)

### Coroutine

[Coroutine](https://en.wikipedia.org/wiki/Coroutine), according to the Wikipedia, allows an execution to be suspended, and resumed from where it was left off. From the code snippet below, we can observe that the coroutine *Gen* **yield**s values at the line 3rd, 5th, and 7th, and the main thread notifies the coroutine by **send** method at the line 17th.

```scala
  1: class Gen extends Coroutine[String, Int] {
  2:   override def generate(): Unit = {
  3:     val received1 = `yield`(1)
  4:     println(s"Message sent from the caller: ${received1}")
  5:     val received2 = `yield`(2)
  6:     println(s"Message sent from the caller: ${received2}")
  7:     val received3 = `yield`(3)
  8:     println(s"Message sent from the caller: ${received3}")
  9:   }
 10: }
 11:
 12: @main def run(): Unit =  {
 13:   val gen = new Gen()
 14:   while (gen.hasMoreElements()) {
 15:     val yielded = gen.nextElement()
 16:     println("Caller receives a value: ${yielded}")
 17:     gen.send(s"Caller sends str ${yielded}")
 18:   }
 19: }
```

Thus, it can be viewed as a generalized subroutine in favor of [cooperative multitasking](https://en.wikipedia.org/wiki/Cooperative_multitasking). A higher level workflow between coroutine(s), and the main thread can be roughly sketeched with the following image.

![Coroutine cooperates with the main thread](images/cooperative-multitasking.png "cooperative multitasking")

### Event loop

Beside the component coroutine, the entire system needs a way to manage the execution of coruotines submitted. A simplest setup is create an event loop that picks up a coroutine from its backlog, and execute that coroutine until the coroutine suspends, or completes. The control flow is then returned to the event loop, which picks up the next coroutine to run, repeating the same operation, until no more pending tasks. The pseudocode could be something like this:

```pseudocode
SET all coroutines TO event loop's backlog somewhere in the system
WHILE event loop's backlog size > 0 DO
  GET a coroutine from event loop's backlog
  EXECUTE the coroutine
  IF running == coroutine state THEN
    PUT the coroutine back to the event loop's backlog
  ELSE IF done == coroutine state THEN
    PASS
  END IF
```

Scala version's code snippet with some detail omitted can be refferred to as below.

* First, the code **fetch**es a task, i.e., coroutine, from its corresponded task queue at the line 5th

* Second, the code **execute**s that task at the line 6th

* Third, the code **check**s the task's state, and act accordingly from the line 7th to 13th - if the task is in *Ready* or *Running* state, the code places the task back to the task queue, continouing the programme by **fetch**ing the next task to run; whereas if the task **accomplish**es its execution, the code repeats the same flow by fetching the next task to run, or the code **exit**s when no more tasks in the task queue

```scala
  1:  def consume(taskQueue: TaskQueue[Task[_, _]]): Any = {
  2:    @tailrec
  3:    def fnWhile(fetchTask: => Task[_, _]...): Any = {
  4:
  5:      val (newTask, ...) = fetchTask
  6:      val (_, newTask1) = execute(newTask)
  7:      newTask1.state() match {
  8:        case State.Ready | State.Running =>
  9:          val (_, ...) = newTaskQueue.add(newTask1)
 10:          fnWhile(newTaskQueue1.fetch())
 11:        case State.Stopped =>
 12:          if (0 != newTaskQueue.size()) fnWhile(newTaskQueue.fetch()) else ()
 13:      }
 14:    }
 15:    fnWhile(taskQueue.fetch())
 16:  }
 17:  scheduler.taskQueues.foreach { taskQueue =>
 18:    val callable = new Callable[Any] {
 19:      @throws(classOf[RuntimeException])
 20:      override def call(): Any = consume(taskQueue)
 21:    }
 22:    executors.submit(callable)
 23:  }
 ```

## Prerequisite

:::caution

The code in the repository merely tests against specific versions of Scala and GraalVM specified in this section. Other versions may or may not be working as expected. Also, at the writeup, build tools such as sbt, maven, and gradle did not work, so some manual operations are necessary.

:::

In order to achieve the goal, following two softwares are required to install before proceeding further - one is GraalVM, the other Scala.

- [GraalVM Espresso: 24.2 standalone](https://www.graalvm.org/latest/reference-manual/espresso)

- Scala 3.3.6

GraalVM provides [Continuation API](https://www.graalvm.org/jdk25/reference-manual/espresso/continuations/), based on [Truffleo framework](https://www.graalvm.org/22.2/graalvm-as-a-platform/language-implementation-framework/), allowing the programme to suspend, resume, and serialize its state to external storages.

### Structure Layout

The layout of the folder structure actually is the same as that of conventional build tools employeed by sbt, maven, and so on.

```bash
async4s
├── async
│   └── src
│       └── main
│           └── scala
│               └── async
│                   └── ... scala files ...
├── continuations
│   └── src
│       └── main
│           └── scala
│               └── continuations
│                   └── Coroutine.scala
└── Makefile
```

### Dependencies

The installation of required depdendent jars can be reffered to at the Makefile [install-async-libraries](https://codeberg.org/chlin501/async4s/src/branch/main/Makefile#L15) target, and [install-continuations-libraries](https://codeberg.org/chlin501/async4s/src/branch/main/Makefile#L33) target. Otherwise, please use the curl command to install those dependencies manully.

```bash
curl -s -L -O --create-dirs --output-dir $(async lib dir) "https://{maven repo website}/.../{scala or continuations}.jar"
```

## Higher Overview

The diagram below sketeches a higher overview relationship between the components this project is going to use.

* *Task* is the model upon which all other components operate

* *Worker* serves as a proxy sitting between *[Scheduler](#scheduler)* and *[Task Queue](#task-queue)*

* *Task Queue* stores [task](#task)s to be executed

* *Scheduler*'s responsibility is to schedule [task](#task)s

* *Runtime* assembles all components together, hosting an environemt for necessary operations

![Async Class Diagram](images/class-diagram.png "Async Class Diagram")

## Components

Primarily, this programme requires following components to work, including *[Task](#task)*, *[Worker](#worker)*, *[Task Queue](#task-queue)*, *[Scheduler](#scheduler)*, and *[Runtime](#runtime)*.

### Task

#### Coroutine

Although GraalVM provdes Continuation API, the API itself does not contain any *Coroutine* classes. Fortunately, this API offers *[Generator](https://www.graalvm.org/latest/reference-manual/espresso/continuations/generators/)* class, which serves several critical methods - `generate()`, `emit()`, `hasMoreElements()`, and `nextElement()`. The first method is a place where a developer fills in their programme's primary logic, inside which the code can execute `emit()` providing a value to the caller if needed, and then suspend the programme execution, whilst the developer can also exploit the last two methods checking if the *Generator* object emits more values inside the caller function.

Therefore, the frist thing is to create a *[Coroutine](https://codeberg.org/chlin501/async4s/src/branch/main/continuations/src/main/scala/continuations/Coroutine.scala#L5)* class. The steps include

1. Adding the [send()](https://codeberg.org/chlin501/async4s/src/branch/main/continuations/src/main/scala/continuations/Coroutine.scala#L8) method (at the line 3th, and 4th) for storing the value sent from the caller side, which will call the *Coroutine* instance during runtime execution

2. Creating the [yield()](https://codeberg.org/chlin501/async4s/src/branch/main/continuations/src/main/scala/continuations/Coroutine.scala#L12) method that will emit a value (at the line 6th) if one or multiple elements to produce, and pass the value (at the line 7th) sent from the caller back to the *Coroutine* if any

```scala
  1: abstract class Coroutine[S, E] extends Generator[E] {
  2:   var value: Option[S] = None
  3:   def send(value: Option[S]): Unit = this.value = value
  4:   def send(value: S): Unit = send(Option(value))
  5:   def `yield`(element: E): Option[S] = {
  6:     emit(element)
  7:     value
  8:   }
  9: }
```

Back to the *Task* component, in fact it is merely a wrapper of the coroutine mentioned above, plus self defined *[State](https://codeberg.org/chlin501/async4s/src/branch/main/async/src/main/scala/async/Task.scala#L20)s* shown as the code snippet annotated with `enum` -

* *Ready* is the initial state during the creation of *Task*

* *Running* denotes this coroutine is in execution at the moment

* *Stopped* means the current coroutine is neither at ready state nor at running state, e.g., the coroutine accomplishes its execution

```scala
  1: enum State {
  2:   case Ready, Stopped, Running
  3: }
```

The *Task* state transition starts from **Ready**, to **Running**, and then to **Stopped**.

With these states, the system can then determine whether to **emit** (at the line 5th, and 11th below), or to **send** (at the line 9th) values during programme execution.

```scala
  1: override def execute(value: Option[S] = None): (Option[E], Task[S, E]) =
  2:   internalState match {
  3:     case State.Ready =>
  4:       if (coroutine.hasMoreElements())
  5:         (Option(coroutine.nextElement()), copy(internalState = State.Running))
  6:       else
  7:         (None, copy(internalState = State.Running))
  8:     case State.Running =>
  9:       value.foreach(coroutine.send(_))
 10:       if (coroutine.hasMoreElements())
 11:         (Option(coroutine.nextElement()), copy(internalState = State.Running))
 12:       else {
 13:         signal.foreach(_.offer(Stopped(name)))
 14:         (None, copy(internalState = State.Stopped))
 15:       }
 16:     case State.Stopped => (None, this)
 18:   }
```

### Worker

When a task is delgated to the worker, it passes that task by `send()` method (at the following line 5th) to its corresponded task queue through a shared channel **tx** - a *[LinkedBlockingQueue](https://docs.oracle.com/en/java/javase/24/docs/api/java.base/java/util/concurrent/LinkedBlockingQueue.html)* class that is [thread-safe](https://docs.oracle.com/en/java/javase/24/docs/api/java.base/java/util/concurrent/BlockingQueue.html#), and [atomicty for queuing methods](https://docs.oracle.com/en/java/javase/24/docs/api/java.base/java/util/concurrent/BlockingQueue.html#). The relationship between *Worker* and *[Task Queue](#task-queue)* is 1 to 1.

```scala
  1: final case class Worker[T](
  2:   name: String,
  3:   tx: LinkedBlockingQueue[T]
  4: ) {
  5:   final def send(task: T): T = { tx.put(task); task }
  6:   ... // other methods
  7: }
```

### Task Queue

The purpose of task queue:

1. Fetch a *Task* from **tx** (at the line 3rd), a channel shared with *[Worker](#worker)*, and an internal list

    1.1. Check the internal list, i.e., **inner**, if containing any *Task*s (at the line 11th)

    1.2. Try fetching with timeout from the shared queue **rx** (at the line 12th) otherwise

    1.3. Blocking fetch from **rx** indfintively (at the line 14th) until a new Task is available

2. Add an unaccomplished *Task* back (at the line 19th) for later execution

```scala
  1: final case class TaskQueue[T](
  2:  name: String,
  3:  rx: LinkedBlockingQueue[T],
  4:  inner: Seq[T] = Seq.empty[T]
  5: ) {
  6:   final def fetch(
  7:     timeout: Duration = 0.seconds,
  8:     blockingFetch: => T = rx.take()
  9:   ): (T, TaskQueue[T]) = {
 10:     val task =
 11:       inner.headOption.getOrElse {
 12:         tryFetch(timeout) match { // tryFetch executes rx.poll(0, TimeUnit.SECONDS) in default
 13:           case Some(taskInQueue) => taskInQueue
 14:           case None              => blockingFetch
 15:         }
 16:       }
 17:     (task, copy(inner = inner.drop(1)))
 18:   }
 19:   final def add(task: T): (T, TaskQueue[T]) =
 20:     (task, copy(inner = task +: inner))
 21:   ... // other methods
 22: }
```

### Scheduler

This is pretty self explanatory, this component manages how a task to be ran. Specifically, the scheduling makes use of *Least-Loaded (LL)* strategy.

#### Least-Loaded (LL) Scheduler

This project exploits least loaded scheduling strategy, which schedules a task to a least loaded worker. The primary reason comes from that the scheduling strategy employed by, e.g., Rust's Tokio work-stealing scheduler is very complicated[1]. Least-Loaded scheduling strategy is simple yet effective[2].

For LL strategy to work, two functions are required:

1. Calculate the range of next batch

    ```scala
      1: final case class LeastLoadedScheduler[T](
      2:    currentRound: Int = 0
      3: ) ...
      4:   final def nextBatch(): (Range, LeastLoadedScheduler[T]) = {
      5:     val prev = currentRound
      6:     val workerLength = workers.length
      7:     val multiple = workerLength + (batchSize - 1)
      8:     val nextMultipleOf = multiple - (multiple % batchSize)
      9:     val next = (prev * batchSize) % nextMultipleOf
     10:     val end = Math.min(next + batchSize, workerLength)
     11:     (Range(next, end), copy(currentRound = currentRound + 1))
     12:   }
    ```


Whilst searching the next batch's range, first keep the value of current round (at the line 5th), and find the length of worker list (at the line 6th).

Second, find the next multiple of value (from the line 7th to 8th). The line 7th by adding `batch size - 1` to **worker length** ensures the obtained number is at least as large as the next multiple, and smaller than the one after that. Then, using that number subtracts the modulo value for acquiring the desired next multiple of value. For instance, with the setting of 22 workers, and batch size 8, the **multiple** value is 29, which is larger than the next multiple value 24, but is smaller than its next multiple of value after 24, which is 32.

Third, calculate the next batch's range. The line 9th makes sure the next value will rorate when exceeding the expected next multiple of number. And the line 10th picks up the minimum value between **workers length**, and `next + batch size`, setting the end of range value to the worker length when the `next + batch size` exceeds the value of **worker length**. Again with the setting of 22 workers, and batch size 8, when the `next + batch size` reaches 24, the logic picks up the worker length 22, which is the maximum worker in the worker list.

Therefore, configuring 22 workers with 8 as its batch size, the range of next batch values in sequence should be (0, 8), (8, 16), (16, 22), and then start over from (0, 8) again.

2. Pick up the lightest loading *Worker*

    ```scala
      1: override def leastLoaded(): (Worker[T], LeastLoadedScheduler[T]) = {
      2:   val (tmpWorkers, newSched) =
      3:     if (workers.length <= batchSize) (workers, this)
      4:     else {
      5:       val (range, sched) = nextBatch()
      6:       (workers.slice(range.start, range.end), sched)
      7:     }
      8:   (tmpWorkers.minBy(_.size()), newSched)
      9: }
    ```


### Runtime

## Conclusions

## References

[1]. [Announcing Nio](https://nurmohammed840.github.io/posts/announcing-nio/)

[2]. [Least-Loaded (LL) Scheduler](https://nurmohammed840.github.io/posts/announcing-nio/#least-loaded-ll-scheduler)
