---
author: gedeon-tabela
category: guide
difficulty: intermediate
excerpt: Learn how to scale and deploy your Scala application to Kubernetes
publishedDate: 2025-03-10
tags: [kubernetes, scala, zero-downtime, devops, orchestration]
title: "Scala applications containers Orchestration with Kubernetes(Part 1)"
updatedDate: 2025-03-11
---

## 1. Introduction: Why Kubernetes Matters for Scala Developers
Imagine that you've built an interesting Scala application, and you are ready to deploy it for your users to consume
your services. As a good developer you foresee numerous real challenges to address for your application to efficiently
serve the public: how will the application scale up and down in response to the traffic, avoid service downtime...?
The most efficient solution for your worries is Kubernetes.

If you as a Scala developer you are struggling with your infrastructure, auto-scaling, deployment headaches, this first
part guide will guide you through building a powerful application ecosystem by introducing you to the core concepts of
a Kubernetes cluster. The second part will guide you through continuous integration and deployment,
monitoring,... and help you build a production-ready kubernetes cluster.

## 2. Kubernetes: Your Application's New Best Friend
Kubernetes is not just another helper tool for deploying applications. It's a complete infrastructural ecosystem positioning
itself as the most modern solution for deploying resilient and elastics applications. It is a hyper-intelligent and extensible
deployment orchestrator that manages all aspects of your application's lifecycle.

### Why Should Scala Developers Care?
Kubernetes promises:
- **Effortless Scaling**: Grow or shrink your application automatically in response to workload
- **Self-Healing Magic**: Automatic response to failing containers during their lifecycle
- **Consistent Deployments**: Run the same application anywhereâ€”local machine, cloud, hybrid environment
- **Resource Optimization**: Squeeze maximum performance from your infrastructure

## 3. The Kubernetes Landscape: A High-Level View
Kubernetes came to life from a Google engineering team drawing on their long experience in managing large-scale containerized
systems. The project was later transferred to the Cloud Native Computing Foundation, establishing it as a community-driven
open-source initiative. Since then the project experience a rapid development and adoption across the industry.
Kubernetes offers as core functionalities:
* Seamless deployment with zero-downtime updates and dynamic scaling
* Complete platform portability across cloud providers and on-premises environments
* Extensible architecture that adapts to specific organizational needs
* Intelligent workload placement and state management
* Built-in self-healing with emerging self-adaptation capabilities
* Comprehensive operational features including storage management, secrets handling, health checks, load balancing, resource
  monitoring, service discovery, and logging.

The kubernetes architecture is made up of two main parts:

### Control Plane (Master nodes)
The control plane administers and make cluster wide decisions such as scheduling, detection and response to cluster events like
starting a new pod when there is a difference between the requirements and the actual state of the cluster. It is also responsible
for monitoring the cluster and making sure the desired state of the cluster is maintained. The main components of hte control plane
are:
* **Kube-apiserver**: responsible for exposing the Kubernetes API, it is the front end of the Kubernetes control plane through
    which we can talk to Kubernetes.
* **etcd** Highly-available consistent key value store for all kubernetes cluster data.
* **Kube-scheduler**: process constantly watching for newly created Pods that are not assigned to a node, select
    a node for them to run base on resource availability distribution cluster-wide.
* **Kube-controller-manager**: responsible for executing **Controllers** processes. In Kubernetes Controllers are control loops
    that watch the state of your cluster through the API-Server, then make or request changes where needed in other to move
    the actual state of your cluster toward the desired state. A Given controller is responsible for tracking one type of
    Kubernetes resource type.
        An example of Controller is the built-in **Job Controller**. A Job is a Kubernetes resource that runs one or multiple
    Pods to execute a task and then stops. When a new task will be scheduled the Job Controller will make sure that some nodes
    on the cluster are running the desired number of Pods to get the work done. It does this by telling the API server to instruct
    the creation of the Pod(container encapsulation that is the smallest unit on kubernetes) that will

### Worker Nodes
The main goal of the worker nodes is to host the Pods that are the components of the application workload. All your application
container instances will be run on the worker nodes. The control plane schedule and manages all activities carry out on the worker
nodes. The main components of the worker nodes are:
* **Kubelet**: an agent running on each node in the cluster and making sure container are running in the Pods as per the
desired state, and are in a good state.
* **Kube-proxy**: a network proxy running on each node in the cluster, responsible for maintaining network rules on nodes. These
network rules allow the communication  to the pods from network sessions inside and outside the cluster.
* **Container runtime**: a software package that knows how to leverage specific features on a supported operating system
to run the containers. Its responsibility is to manage the execution of containers withing the kubernetes environment.


### Development environment
For the development environment we'll use minikube as our local kubernetes. It is a solution that make easy learning and
development easier for kubernetes. For it to work, you mainly need docker installed. After this all you need is to follow
the instructions on the page (https://minikube.sigs.k8s.io/docs/start)[Install k8s for local development].
Kubernetes specifications artifact are written in yaml format and all the kubernetes components details are found on the
link (Kubernetes components)[https://kubernetes.io/docs/concepts/]. For the following deployment scripts we'll use these
two commands to deploy and drop kubernetes components on our cluster, where `workload-api.yaml` is a kubernetes yaml
configuration file, and kubectl is the client we use to send our request to the API-Server:
Deployment: `kubectl apply -f workload-api.yaml`
Dropping the deployment: `kubectl delete -f workload-api.yaml`
To have a view of the kubernetes internals we can run in a separate terminal the commands `minikube addons enable dashboard` and
` kubectl addons enable metrics-server` that will install the needed tools to visualize the activity on the cluster.
We can then run the command `minikube dashboard` which will open a web page in our default browser to see all the objects
running on the cluster.

## 4. Kubernetes Components: Meet the Ecosystem

### Pods: Your Application's Smallest Unit
Think of a Pod like a dedicated apartment for your containers. It's a compact living space where one or more containers
live together, sharing resources. Imagine a Pod as a smart, self-contained shipping container for your application.
It's not just a simple container, but a sophisticated ecosystem with multiple layers of intelligence behind its creation
and deployment.

#### Pod Lifecycle and Scheduling: Behind the Scenes
In kubernetes, Pods are the smallest unit of workload execution that you can create and manage. It is a group of one or
more containers, with shared storage and network resources and specification for how to run the container.

When you create a Pod, Kubernetes initiates multiple actions:

* **Scheduling Decision**: The scheduler receives pod creation request and base on the current state of the cluster,
    the available computing resources the cluster topology requirements, Hardware constraints and other factors selects
    the node on which the pod will run.
** **Pod Placement Workflow**: After the Scheduler has identified the optimal node on which the pod will run,
    the Control plane communicates with the selected node's Kubelet and give all the specification for running the pod.
    The Kubelet then pulls necessary container images and the container runtime create and starts containers. At this step
    the Pod is said to be in "Running" state after a successful initialization.
* **Container Startup Sequence**: If the init containers have been provided, they will run first and only after that the
    pod main container will start. If liveness and readiness probes are provided, pod will become available only after they
    are successful and will then be made available for service discovery and networking.

* **Notification**: After the pod are created, Kubelet will reports Pod creation status to the API Server, and confirm
    successful container deployment. After this the cluster state in the ectd key-value store database will be updated.

##### Visualization of the Process
Here's a sequential diagram to illustrate the workflow:
![Pod lifecycle](images/kubernetes-pod-lifecycle-sequential-diagram.png)

##### Example
```yaml
apiVersion: v1
kind: Pod
metadata:
 name: workload-api
 labels:
    type: api
    language: scala
    framework: play-framework
    service: workload
spec:
 containers:
   - name: workload-api
     image: aidocking/pfpw:20.20.31
     ports:
       - containerPort: 9000
```
Deploy: `kubectl apply -f workload-api.yaml`
![Pods creation result](images/pod-created.png)
View: `kubectl get pods` or `kubectl get -f workload-api.yaml`
![Visualizing the pod created](images/pod-applied.png)
Drop: `kubectl delete -f workload-api.yaml` or `kubectl delete pod workload-api`
![Result of dropping the pod](images/pod-deletion-result.png)

Explanations:
* **API Version and Kind**: The first two lines specify the Kubernetes Pods API version, which is crucial for Kubernetes
  to understand the intended action and correctly process the Pod creation request.
* **Metadata Section*: This part provides descriptive information about the Pod, including its name and labels. While
  labels don't affect the Pod's behavior currently, they become important when working with Kubernetes controllers.
* **Specification Section**: The spec defines the container(s) within the Pod. In this example, we're using a single
  container, but Kubernetes allows multiple containers to be defined in a single Pod configuration.
* **Container Definition**: The container details include its name, the Docker image to use, and the specific command to
  run when the container starts up. In this case, it's a Play framework app container with the port 9000 exposed.
It should be noted that pod are not intended to be created individually in real-life scenarios. They will be created through
deployments.

### ReplicaSet
The primary function of a ReplicaSet is to ensure that a specified number of pods replicas is almost always running.

#### ReplicaSet Lifecycle: A Step-by-Step Symphony
When the user or system triggers ReplicaSet creation Kubernetes client (kubectl) sends a ReplicaSet creation request
to the API server and the payload includes detailed specifications.
When the kubernetes controller detects new ReplicaSet object in the cluster, it initiates a reconciliation process
by Comparing the desired state (specified replicas) with current cluster state. This will be followed by the pod creation
mechanism as described above.

#### Visualization of the Process
![ReplicaSet lifecycle](images/kubernetes-replicaset-lifecycle-sequence-diagram.png)


#### Example
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: workload-api-rep
spec:
  replicas: 2
  selector:
    matchLabels:
      type: api
      language: Scala
      framework: play-framework
  template: # Pod definition
    metadata:
      labels:
        type: api
        language: Scala
        framework: play-framework
    spec:
      containers:
        - name: workload-api
          image: aidocking/pfpw:20.20.31
          resources:
            limits:
              memory: "3Gi"
              cpu: "2"
```
Deploy: `kubectl apply -f workload-api.yaml`
![Pods replicaset creation result](images/replicaset-creation-result.png)
View: `kubectl get replicasets` or `kubectl get -f workload-api.yaml`
![Visualizing the replicaset created](images/replicaset-created.png)
Drop: `kubectl delete -f workload-api.yaml` or `kubectl delete replicaset workload-api-rep`
![Dropping result](images/replicaset-deletion-result.png)

Explanations:
* **API and Resource Definition**: Specify the Kubernetes API version for a ReplicaSet, defining its basic metadata with
  a name identifier.

* **Replica Configuration**: Set the desired number of concurrent Pods, defaulting to one if not explicitly specified.
  This ensures consistent application deployment across the cluster.

* **Pod Selection Mechanism**: Use a selector to identify and manage Pods, regardless of their origin. The ReplicaSet
  dynamically maintains the specified number of Pods by creating, monitoring, or terminating instances as needed.

* **Label Matching Strategy**: Define precise label criteria to match and manage Pods, ensuring that the ReplicaSet
  targets the correct set of containers across the Kubernetes environment. In this case pod having the labels
  type: api, language: Scala and framework: play-framework will be managed by the ReplicaSet

* **Template Specification**: Provide a Pod template that serves as a blueprint for creating new Pods when necessary,
  with mandatory container definitions that guide the ReplicaSet's deployment logic.

* **Container Definition**: Specify the containers to be deployed within the Pods, detailing their configuration and
  ensuring consistent application setup across replicas. we specify a name, image and limits of the resources consumed
  by the containers in pods

### Services: The Communication Architects

Services are like sophisticated phone switchboards, routing traffic to the right containers and providing a
stable network endpoint. In the complex world of Kubernetes, Services are the master network conductors,
transforming the ephemeral and dynamic nature of Pods into stable, predictable communication endpoints.
They provide stable addresses for communication while pod network addresses are lost when a pod is killed and
new one are assigned when pods are created and started. Imagine Pods as temporary workers constantly moving between offices.
Services are like permanent phone numbers that always route to the right person, regardless of their current
location. Without Services, networking in Kubernetes would be chaotic and unreliable.

#### Service Types: Navigating Communication Strategies

1. **ClusterIP Service**: This is the backbone of cluster Internal Communication, and it is the default service type.
    It provides the stable IP address inside the cluster and is accessible internally. It is best suited for inter service
    communication, and internal networking.

    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: workload-service
    spec:
      type: ClusterIP
      selector:
        type: api
        language: scala
        framework: play-framework
        service: workload
      ports:
       - port: 9000
         targetPort: 9000
    ```
    Deploy: `kubectl apply -f workload-api.yaml`
    ![cluster ip creation result](images/service-created-result.png)
    View: `kubectl get services` or `kubectl get -f workload-api.yaml`
    ![Visualizing the replicaset created](images/service-cluster-ip.png)
    Drop: `kubectl delete -f workload-api.yaml` or `kubectl delete service workload-service`
    ![Dropping result](images/service-deletion.png)

2. **NodePort Service**: This service type offer and external Access Gateway by exposing a service on a static port(30000-32767)
    across all nodes. With this type of service you can make your application accessible outside the cluster. this type is best suited
    for development and testing environments.

   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: workload-service
   spec:
     type: NodePort
     selector:
       type: api
       language: scala
       framework: play-framework
     ports:
       - port: 9000
         targetPort: 9000
         nodePort: 30123
   ```
    Deploy: `kubectl apply -f workload-api.yaml`
    ![service ClusterIP creation result](images/service-created-result.png)
    View: `kubectl get services` or `kubectl get -f workload-api.yaml`
    ![Visualizing the replicaset created](images/service-node-port-view.png)
    Drop: `kubectl delete -f workload-api.yaml` or `kubectl delete service workload-service`
    ![Dropping result](images/service-deletion.png)

3. **LoadBalancer Service**: This type exposes the service externally using  an external load balancer
   generally provided by your cloud provider. It is best suited for production ready web applications, public-facing services,
   and integration with external systems.

   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: workload-service
   spec:
     type: LoadBalancer
     selector:
        type: api
        language: scala
        framework: play-framework
        service: workload
     ports:
       - port: 9000
         targetPort: 9000
   ```
    Deploy: `kubectl apply -f workload-api.yaml`
    ![LoadBalancer service creation result](images/service-created-result.png)
    View: `kubectl get services` or `kubectl get -f workload-api.yaml`
    ![Visualizing the LoadBalancer service created](images/service-load-balancer-created.png)
    Drop: `kubectl delete -f workload-api.yaml` or `kubectl delete service workload-service`
    ![Dropping result](images/service-deletion.png)

4. **ExternalName Service**: this type of service maps the service to an external DNS name, like a database hosted outside
    the cluster. It then helps as an abstraction layer for external resources. The key advantages of using services over direct pod
    communication is that  services survive to Pod restarts, and decouple service discovery from pod lifecycle. When a pod restarts,
    the previous network address is lost and a new one is assigned to the new pod. It also serves as a load balancer distributing
    traffic across multiple pods.

   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: workload-external-database
   spec:
     type: ExternalName
     selector:
       type: api
       language: scala
       framework: play-framework
       service: workload
     externalName: workload-database.example.com
   ```
    Deploy: `kubectl apply -f workload-api.yaml`
    ![ExternalName service creation result](images/service-extenal-name-created.png)
    View: `kubectl get services` or `kubectl get -f workload-api.yaml`
    ![Visualizing the LoadBalancer service created](images/service-ext-name-created-view.png)
    Drop: `kubectl delete -f workload-api.yaml` or `kubectl delete service workload-service`
    ![Dropping result](images/ext-name-serv-deletion-result.png)

#### Visualization: Service Communication Flow
![Service lifecycle](images/kubernetes-service-lifecycle-sequential-diagram.png)

#### Example
In this example, a Scala play framework application is being served through a NodePort service type.
```yaml
apiVersion: v1
kind: Pod
metadata:
 name: workload-api
 labels:
   type: api
   language: scala
   framework: play-framework
   service: workload
spec:
 containers:
   - name: workload-api
     image: aidocking/pfpw:20.20.31
     ports:
       - containerPort: 9000

---
apiVersion: v1
kind: Service
metadata:
 name: workload-service
spec:
 type: NodePort
 selector:
   type: api
   language: scala
   framework: play-framework
 ports:
   - port: 9000
     targetPort: 9000
     nodePort: 30123
# Deploy: kubectl apply -f workload-api.yaml service-and-pod-cration-result.png
# View: kubectl get -f workload-api.yaml
# Get the cluster ip: minikube ip
# access the service: clusterIp:nodePort (clusterIp:30123)
# Drop: kubectl delete -f workload-api.yaml
```
Deploy: `kubectl apply -f workload-api.yaml`
![Service serving pod creation result](images/service-node-creation-result.png)
View: `kubectl get -f workload-api.yaml`
![Visualizing service and pod created](images/view-service-and-pod.png)
Drop: `kubectl delete -f workload-api.yaml` or `kubectl delete service workload-service`
![Dropping result](images/service-and-pod-deletion.png)

Explanations:
* **API and Resource Definition:** Specify the Kubernetes API version for a ReplicaSet, defining its basic metadata with a name identifier.

* **Replica Configuration:** Set the desired number of concurrent Pods, defaulting to one if not explicitly specified. This ensures consistent application deployment across the cluster.

* **Pod Selection Mechanism:** Use a selector to identify and manage Pods, regardless of their origin. The ReplicaSet dynamically maintains the specified number of Pods by creating, monitoring, or terminating instances as needed.

* **Label Matching Strategy:** Define precise label criteria to match and manage Pods, ensuring that the ReplicaSet targets the correct set of containers across the Kubernetes environment.

* **Template Specification:** Provide a Pod template that serves as a blueprint for creating new Pods when necessary, with Kubernetes requires an external mechanism to manage incoming web traffic properly. This Ingress system needs to route requests from standard web ports to the appropriate internal services based on URL paths and domains, while also handling SSL encryption. Unlike other controllers built into Kubernetes core, Ingress controllers must be installed separately - Kubernetes only provides the API framework through its Ingress resource definition. Fortunately, the community has developed numerous Ingress controller implementations to fulfill these requirements, though selection depends on specific project needs and infrastructure constraints.mandatory container definitions that guide the ReplicaSet's deployment logic.

* **Container Definition:** Specify the containers to be deployed within the Pods, detailing their configuration and ensuring consistent application setup across replicas.


### Ingress

Kubernetes requires an external mechanism to manage incoming web traffic properly. This Ingress system needs to route requests
from standard web ports to the appropriate internal services based on URL paths and domains, while also handling SSL encryption.
Unlike other controllers built into Kubernetes core, Ingress controllers must be installed separately - Kubernetes only provides
the API framework through its Ingress resource definition. Fortunately, the community has developed numerous Ingress controller
implementations to fulfill these requirements, though selection depends on specific project needs and infrastructure constraints.
The first thing to do is to enable ingress controller in our cluster: `kubectl addons enable ingress`

#### Example
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: workload-api
  labels:
    type: api
    language: scala
    framework: play-framework
    service: workload
spec:
  containers:
    - name: workload-api
      image: aidocking/pfpw:20.20.31
      ports:
        - containerPort: 9000

---
apiVersion: v1
kind: Service
metadata:
  name: workload-service
spec:
  type: ClusterIP
  selector:
    type: api
    language: scala
    framework: play-framework
  ports:
    - port: 9000
      targetPort: 9000
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-workload-ingress
  annotations:
    'spec.ingressClassName': "nginx"
    ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
    - host: k8splayworkload.com
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: workload-service
                port:
                  number: 9000
```
Deploy: `kubectl apply -f workload-api.yaml`
![Ingress and service creation result](images/ingress-service-and-pod-creation.png)
View: `kubectl get -f workload-api.yaml`
![Visualizing ingress service and pod created](images/ingress-service-and-pod-view.png)
Drop: `kubectl delete -f workload-api.yaml` or `kubectl delete ingress api-workload-ingress`
![Dropping result](images/ingress-service-and-pod-deletion.png)

Explanations:
The metadata section introduces annotations as a flexible mechanism for providing controller-specific information beyond
the core Ingress specification. This design intentionally keeps the Kubernetes Ingress API minimal while enabling
controller-specific customization through annotations. `k8splayworkload.com` is the fully qualified domain name of a
network host, as defined by RFC 3986  We disable HTTPS redirection with the annotation
`nginx.ingress.kubernetes.io/ssl-redirect: false` since we lack SSL certificates for our exercises. In the specification
section, we define traffic routing rules. Our configuration routes all HTTP requests with paths beginning with `/` to the
`workload-service` service on port 8080, creating a clean pathway for external traffic to reach our specific application.
backend defines the referenced service endpoint to which the traffic will be forwarded to in this case `workload-service`.
To access the service trough the nginx ingress controller open your `/etc/hosts` in a linux machine(or corresponding file on
other os) and append this line: `node-address k8splayworkload.com` where node-id is the address of your cluster node that
can be retrieve using `minikube ip`. After this the targeted service becomes accessible from your browser at the hostname
`k8splayworkload.com`.

### ConfigMaps and Secrets: Configuration Maestros

Separate your configuration from code! ConfigMaps handle non-sensitive configurations, while Secrets manage sensitive
data like passwords and tokens here we have converted the actual value into base64 using:
`echo -n value-of-secret | base64 --decode`. Replace in this command the `value-of-secret` with the value of
your secret. `config-one`(respectively `secret-one`) are the keys and `config-value` (respectively `secret-one`...) are
values.

```yaml
# ConfigMap for application settings
apiVersion: v1
kind: ConfigMap
metadata:
 name: workload-config
data:
 config-one: "config-value"
```
Deploy: `kubectl apply -f workload-api.yaml`
![ConfigMaps result](images/config-map-created-result.png)
View: `kubectl get -f workload-api.yaml` or `kubectl get configmaps`
![Visualizing config map created](images/view-config-map-created.png)
Drop: `kubectl delete -f workload-api.yaml` or `kubectl delete configmap workload-config`
![Dropping result](images/delete-configmap-result.png)

```yaml
# Secrets for sensitive information
apiVersion: v1
kind: Secret
metadata:
 name: workload-secret
type: Opaque
data:
 secret-one: bW9uZ291c2Vy
 secret-two: bW9uZ29wYXNzd29yZA==
```
Deploy: `kubectl apply -f workload-api.yaml`
![Secrets result](images/secret-creation-result.png)
View: `kubectl get -f workload-api.yaml` or `kubectl get configmaps`
![Visualizing secret created](images/secret-view.png)
Drop: `kubectl delete -f workload-api.yaml` or `kubectl delete secret workload-secret`
![Dropping result](images/secret-deleted-result.png)

### Deployments: Your Rollout Strategy Manager

Deployments are the smart project managers of Kubernetes. They handle updates, rollbacks, and ensure smooth transitions
between application versions. Kubernetes Deployments offer sophisticated orchestration capabilities that eliminate
downtime during application updates. This mechanism handles the complex coordination required when transitioning between
software versions by carefully managing Pod lifecycles and traffic routing. When initiated, a Deployment creates a new
ReplicaSet that gradually scales up with updated application instances while simultaneously scaling down the previous
version's ReplicaSet. This controlled transition ensures continuous service availability as traffic seamlessly shifts
to new Pods only after they're fully operational. The process includes built-in health checks, rollback capabilities,
and configurable deployment strategies (like rolling updates with customizable surge parameters), effectively preventing
the service disruptions commonly experienced with traditional update methods. This comprehensive approach ensures
applications remain available throughout the entire update lifecycle, maintaining system reliability even during active
changes.


#### Deployment Lifecycle: A Step-by-Step Symphony

1) * **Initiation** : The user submits a Deployment YAML to the API Server, which validates and stores it.
2) * **Controller Actions** :
    * The Deployment Controller in the Controller Manager creates a ReplicaSet
    * The ReplicaSet Controller then creates Pod objects to match the desired count
3) * **Scheduling Process** : The Scheduler assigns Pods to specific Nodes based on resource requirements and constraints
4) * **Pod Creation** :
    * The Kubelet on the assigned Node pulls container images
    * Container Runtime creates and starts the actual containers
    * Status updates flow back through the system
5) * **Monitoring Cycle** :
    * The Kubelet monitors container health and restarts containers if needed
    * The Controllers ensure the desired state is maintained

#### Visualization of the deployment process
This diagram illustrates how Kubernetes orchestrates deployments through multiple coordinated components, each with
specific responsibilities in the workflow.
![Deployment lifecycle](images/kubernetes-deployment-sequence-diagram.png)


```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workload-api
  labels:
    demo: k8s-auto-scaling
spec:
  selector:
    matchLabels:
      type: api
      language: scala
      framework: play-framework
      service: workload
  minReadySeconds: 1
  progressDeadlineSeconds: 1200
  revisionHistoryLimit: 5
  strategy: # Deployment strategy
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template: # Pod definition
    metadata:
      labels:
        type: api
        language: scala
        framework: play-framework
        service: workload
    spec:
      containers:
        - name: workload-api
          image: aidocking/pfpw:20.20.31
          ports:
            - containerPort: 9000
          livenessProbe:
            httpGet:
              path: /live
              port: 9000
            failureThreshold: 3
            timeoutSeconds: 2
            periodSeconds: 10
            initialDelaySeconds: 5

          startupProbe:
            httpGet:
              path: /startup
              port: 9000
            failureThreshold: 30
            periodSeconds: 15
            timeoutSeconds: 2

          readinessProbe:
            httpGet:
              path: /ready
              port: 9000
            failureThreshold: 10
            periodSeconds: 15
            timeoutSeconds: 10

---
apiVersion: v1
kind: Service
metadata:
  name: workload-service
spec:
  type: ClusterIP
  selector:
    type: api
    language: scala
    framework: play-framework
  ports:
    - port: 9000
      targetPort: 9000
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-workload-ingress
  annotations:
    'spec.ingressClassName': "nginx"
    ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
    - host: k8splayworkload.com
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: workload-service
                port:
                  number: 9000
```
Deploy: `kubectl apply -f workload-api.yaml`
![Secrets result](images/deployment-creation-result.png)
View: `kubectl get -f workload-api.yaml`
![Visualizing secret created](images/view-full-deployment.png)
Drop: `kubectl delete -f workload-api.yaml`
![Dropping result](images/deployment-deletion-result.png)

Explanations:
* **minReadySeconds**: let us specify the minimum number of seconds to wait before kubernetes starts considering the pods
  ready.
* **progressDeadlineSeconds**: The maximum time in seconds for a deployment to make progress before it is considered to be failed
* **revisionHistoryLimit**: Defines the number of previous replicas to track so that we'll be able to roll back to any
  of those previous ReplicaSets.
* **strategy**: define a way to replace running pod with new one having another version. There are two types: RollingUpdate
  or Recreate. Recreate terminate all the pods before an update(new image to deploy). Rolling update progressively replace
  new old pod with new ones. maxSurge specifies the maximum number of pods that can be scheduled above the desired number
  of pods. maxUnavailable specifies the maximum number of pods that can be unavailable during the update.
* **readinessProbe**: Periodic probe of container service readiness. Container will be removed from service endpoints if the probe fails
* **startupProbe**: StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are executed until
  this completes successfully. If this probe fails, the Pod will be restarted, just as if the livenessProbe failed.
* **livenessProbe**: Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated

### A complete example from all the previous snippets

```yaml
# Secret
apiVersion: v1
kind: Secret
metadata:
  name: workload-secret
type: Opaque
data: # values have been encoded in base64 using `echo -n  data | base64`
  secret-one: bW9uZ291c2Vy
  secret-two: bW9uZ29wYXNzd29yZA==

# ConfigMap
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: workload-config
data:
  config-one: "config-value"

# Deployment
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: workload-api
  labels:
    demo: k8s-auto-scaling
spec:
  selector:
    matchLabels:
      type: api
      language: scala
      framework: play-framework
      service: workload
  minReadySeconds: 1
  progressDeadlineSeconds: 1200
  revisionHistoryLimit: 5
  strategy: # Deployment strategy
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template: # Pod definition
    metadata:
      labels:
        type: api
        language: scala
        framework: play-framework
        service: workload
    spec:
      containers:
        - name: workload-api
          image: aidocking/pfpw:20.20.31
          ports:
            - containerPort: 9000
          env:
            - name: CONFIG_ONE
              valueFrom:
                configMapKeyRef:
                  name: workload-config
                  key: config-one
            - name: SECRET_ONE
              valueFrom:
                secretKeyRef:
                  name: workload-secret
                  key: secret-one

          livenessProbe:
            httpGet:
              path: /live
              port: 9000
            failureThreshold: 3
            timeoutSeconds: 2
            periodSeconds: 10
            initialDelaySeconds: 15

          startupProbe:
            httpGet:
              path: /startup
              port: 9000
            failureThreshold: 30
            periodSeconds: 15
            timeoutSeconds: 2
            initialDelaySeconds: 45

          readinessProbe:
            httpGet:
              path: /ready
              port: 9000
            failureThreshold: 10
            periodSeconds: 15
            timeoutSeconds: 10

          resources:
            limits:
              memory: "2.5Gi"
              cpu: "2"

# api service
---
apiVersion: v1
kind: Service
metadata:
  name: workload-service
  labels:
    service: workload
spec:
  type: ClusterIP
  selector:
    type: api
    language: scala
    framework: play-framework
    service: workload
  ports:
    - port: 9002
      protocol: TCP
      targetPort: 9000

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-workload-ingress
  annotations:
    'spec.ingressClassName': "nginx"
    ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
    - host: k8splayworkload.com
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: workload-service
                port:
                  number: 9002

---
# workload auto-scaling strategy
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: workload-horizontal-autoscaling
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: workload-api
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 75
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 75
  behavior:
    scaleUp:
      policies:
        - periodSeconds: 60
          type: Percent
          value: 10
      selectPolicy: Max
      stabilizationWindowSeconds: 120
    scaleDown:
      policies:
        - periodSeconds: 60
          type: Percent
          value: 1
        - type: Pods
          value: 1
          periodSeconds: 60
      selectPolicy: Max
      stabilizationWindowSeconds: 120
```

Deploy: `kubectl apply -f workload-api.yaml`
![Full example](images/full-example-creation-result.png)
View: `kubectl get -f workload-api.yaml`
![Visualizing full example](images/full-example-view.png)
Drop: `kubectl delete -f workload-api.yaml`
![Dropping result](images/full-example-deletion-results.png)

#### Auto-scaling
We have added a horizontal auto-scaling policy for this deployment.
* **minReplicas** is the lower limit for the number of replicas to which the auto-scaler can scale down. It defaults to 1 pod.
* **maxReplicas** is the upper limit for the number of replicas to which the auto-scaler can scale up. It cannot be less that minReplicas.
* **metrics** contains the specifications for which to use to calculate the desired replica count (the maximum replica count across all metrics will be used
* **metrics.type** is the type of metric source
* **metrics.type.resource**  refers to a resource metric (such as those specified in requests and limits) known to
  Kubernetes describing each pod in the current scale target (e.g. CPU or memory)
* **metrics.resource.name** is the name of the resource in question.
* **metrics.resource.target** specifies the target value for the given metric.
* **metrics.resource.target.type** represents whether the metric type is Utilization, Value, or AverageValue.
* **metrics.resource.target.averageUtilization** is the target value of the average of the resource metric across all
  relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type.

Using this bash script you can overload your scala play framework application running in the pod, and see how the
kubernetes auto-scale the application in response to the incoming load. Adjust the frequencies per minutes and the duration
of the overload in minutes.

```bash
#!/bin/bash

# Configuration variables.
# Memory workload configs
#d: duration of the memory hold(seconds)
#m: total memory to hold(MB)
#s: memory chunks to allocate progressively(MB)
#l: flag to enable logging (default: true)
# Adjust the parameters according to your memory/cpu capacity to get the desired effect
# The test was done on an Intel 2.4GHZ CPU
ENDPOINT="http://k8splayworkload.com/createMemoryLoad?d=180&m=512&s=28&l=true"  # Target MEMORY WORKLOAD HTTP endpoint
# CPU workload config: n is the order of fibonacci to compute
#ENDPOINT="http://k8splayworkload.com/fibonacci?n=4569879"  # Target CPU WORKLOAD HTTP endpoint
CALLS_PER_MINUTE=70                # Number of calls per minute
TOTAL_MINUTES=3                    # Total duration in minutes

# Display configuration
echo "Starting HTTP load generator with the following configuration:"
echo "Endpoint: $ENDPOINT"
echo "Frequency: $CALLS_PER_MINUTE calls per minute"
echo "Duration: $TOTAL_MINUTES minutes"
echo "Total calls to be made: $((CALLS_PER_MINUTE * TOTAL_MINUTES))"
echo "----------------------------------------"

# Calculate timing
SLEEP_TIME=$(bc <<< "scale=6; 60 / $CALLS_PER_MINUTE")
TOTAL_SECONDS=$((TOTAL_MINUTES * 60))
START_TIME=$(date +%s)
END_TIME=$((START_TIME + TOTAL_SECONDS))

# Initialize counter
COUNTER=0

# Run the load test
while [ $(date +%s) -lt $END_TIME ]; do
  # Make HTTP request without waiting for response
  curl -s -o /dev/null -w "" "$ENDPOINT" &

  # Increment counter
  COUNTER=$((COUNTER + 1))

  # Display progress every 10 requests
  if [ $((COUNTER % 10)) -eq 0 ]; then
    ELAPSED_TIME=$(($(date +%s) - START_TIME))
    ELAPSED_MINUTES=$(bc <<< "scale=2; $ELAPSED_TIME / 60")
    PERCENT_COMPLETE=$(bc <<< "scale=2; ($ELAPSED_TIME / $TOTAL_SECONDS) * 100")
    echo "Progress: $COUNTER requests sent ($ELAPSED_MINUTES minutes elapsed, $PERCENT_COMPLETE% complete)"
  fi

  # Sleep for calculated time
  sleep $SLEEP_TIME
done

# Final report
TOTAL_TIME=$(($(date +%s) - START_TIME))
ACTUAL_RATE=$(bc <<< "scale=2; $COUNTER / ($TOTAL_TIME / 60)")

echo "----------------------------------------"
echo "Load test completed:"
echo "Total requests sent: $COUNTER"
echo "Actual duration: $(bc <<< "scale=2; $TOTAL_TIME / 60") minutes"
echo "Average rate: $ACTUAL_RATE requests per minute"
```
Having an eye on the kubernetes dashboard page you can see how the cluster reacts in response to the
load. You'll notice how it scales by adding the number of pods and later when the overload is passed it
scales down.

#### Zero down-time deployment
We have defined a zero-downtime deployment in the attribute `strategy`. to see how this works we can change the actual version
of the app that is running, and we'll see that kubernetes does not stop all the running container before starting new ones. Instead,
it progressively changes old instances(`pfpw:20.20.31`) with new instances having the new version/release(`pfpw:20.20.32`). Doing this there will be no time when
the service will not be available. To apply a new version run this command:
`kubectl set image -f workload-api.yml workload-api=aidocking/pfpw:20.20.32`

## Conclusion: Your Kubernetes Journey Begins

Kubernetes isn't just a tool, it's a mindset. For Scala developers, it represents a powerful way to transform how we
think about deploying and managing applications. Start small, experiment, and gradually build your Kubernetes expertise.
The learning curve is steep, but the view from the top is spectacular!

