---
benefits:
  hours: 8
  linesOfCode: 1400
category: spark
description: Learn advanced Spark performance tuning for ultimate speed. Master Spark internals and configurations to maximize the efficiency of your cluster.
excerpt: <p><strong>Tune Apache Spark for best performance.</strong> Master Spark internals and configurations for maximum speed and memory efficiency for your cluster.</p>
faqs:
  - question: How long is the course? Will I have time for it?
    answer: The course is almost 8 hours in length, with lessons usually 20-30 minutes each, and we write 1000-1500 lines of code. That’s because to learn strategies to boost Spark’s performance, 5-minute lectures or fill-in-the-blanks quizzes won’t give you the necessary results. For the best effectiveness, it’s advised to watch the video lectures in 1-hour chunks at a time.
  - question: What does a typical lesson contain?
    answer: Code is king, and we write from scratch. In a typical lesson I'll explain some concepts in short, then I'll dive right into the code. We'll write it together, either in the IDE or in the Spark Shell, and we test the effects of the code on either pre-loaded data (which I provide) or with bigger, generated data (whose generator I also provide). Sometimes we'll spend some time in the Spark UI to understand what's going on. A few lectures are atypical in that we're going to go through some thought exercises, but they're no less powerful.
  - question: Can I expense this at my company?
    answer: A wise company will spend some money on training their folks here rather than spending thousands (or millions) on computing power for nothing.
  - question: Is this hard?
    answer: There's a reason not everyone is a Spark pro. However, my job is to give you these (otherwise hard) topics in a way that will make you go like "huh, that wasn't so hard".
  - question: What if I'm not happy with the course?
    answer: If you're not 100% happy with the course, I want you to have your money back. It's a risk-free investment.
  - question: Daniel, I can't afford the course. What do I do?
    answer: For a while, I told everyone who could not afford a course to email me and I gave them discounts. But then I looked at the stats. Almost ALL the people who actually took the time and completed the course had paid for it in full. So I'm not offering discounts anymore. This is an investment in yourself, which will pay off 100x if you commit. If you find it didn't match your investment, I'll give you a refund.
  - question: I have very little Scala or Spark experience. Can I take this course?
    answer: "Short answer: no. Long answer: we have two recap lessons at the beginning, but they're not a crash course into Scala or Spark and they're not enough if this is the first time you're seeing them. You should take the Scala beginners course and the Spark Essentials course at least. I'll also recommend taking the first Spark Optimization course, but it's not a requirement - this course is standalone. "
  - question: Why should I need to tune Spark?
    answer: Spark comes with a lot of performance tradeoffs that you will have to make while running your jobs. It's important to know what they are and how you can use each configuration or setting, so that you can get the best performance out of your jobs.
  - question: What is Spark performance tuning anyway?
    answer: To get the optimal memory usage and speed out of your Spark job, you will need to know how Spark works. Tuning Spark means setting the right configurations before running a job, the right resource allocation for your clusters, the right partitioning for your data, and many other aspects.
image: images/spark-performance-tuning.png
price: 75
purchaseLink: https://rockthejvm.com/purchase?product_id=3428855
title: Spark Performance Tuning with Scala
pricingPlanId: 3428855
---

import CourseLayout from "@pages/courses/_layouts/CourseLayout.astro";

<CourseLayout>
  <Fragment slot="goal">
    ### They say Spark is fast. How do I make the best out of it?

    I wrote a lot of Spark jobs over the past few years. Some of my old data pipelines are probably still running as you're reading this. However, my journey with Spark had massive pain. You've probably seen this too.

    - <p>You run 3 big jobs with the same DataFrame, so you try to cache it - but then you look in the UI and it's nowhere to be found.</p>
    - <p>You're finally given the cluster you've been asking for... and then you're like "OK, now how many executors do I pick?".</p>
    - <p>You have a simple job with 1GB of data that takes 5 minutes for 1149 tasks... and 3 hours on the last task.</p>
    - <p>You have a big dataset and you know you're supposed to partition it right, but you can't pick a number between 2 and 50000 because you can find good reasons for both!</p>
    - <p>You search for "caching", "serialization", "partitioning", "tuning" and you only find obscure blog posts and narrow StackOverflow questions.</p>
    Unless you have some massive experience or you're a Spark committer, you're probably using 10% of Spark capabilities.

    In the Spark Optimization course you learned how to write performant code. It's time to kick the high gear and tune Spark for the best it can be. You are looking at the only course on the web which leverages Spark features and capabilities for the best performance. With the techniques you learn here you will save time, money, energy and massive headaches.

    ### Let's rock.

    In this course, we cut the weeds at the root. We dive deep into Spark and understand what tools you have at your disposal - and you might just be surprised at how much leverage you have. **You will learn 20+ techniques for boosting Spark performance. Each of them individually can give at least a 2x perf boost for your jobs (some of them even 10x), and I show it on camera.**

  </Fragment>
  <Fragment slot="skills">
    ### What's in for you:
    - <p>You'll understand Spark internals to explain how Spark is already pretty darn fast</p>
    - <p>You'll be able to predict in advance if a job will take a long time</p>
    - <p>You'll diagnose hanging jobs, stages and tasks</p>
    - <p>You'll spot and fix data skews</p>
    - <p>You'll make the right performance tradeoffs between speed, memory usage and fault-tolerance</p>
    - <p>You'll be able to configure your cluster with the optimal resources</p>
    - <p>You'll save hours of computation time in this course alone (let alone in prod!)</p>
    - <p>You'll control the parallelism of your jobs with the right partitioning</p>

    <p>And some extra perks:</p>
    - <p>You'll have access to the entire code I write on camera (~1400 LOC)</p>
    - <p>You'll be invited to our private Slack room where I'll share latest updates, discounts, talks,</p>
    - <p>(Soon) You'll have access to the takeaway slides</p>
    - <p>(Soon) You'll be able to download the videos for your offline view</p>

    ### Skills you'll get:

    - <p>Deep understanding of Spark internals so you can predict job performance</p>
      - <p>stage & task decomposition</p>
      - <p>reading query plans before jobs will run</p>
      - <p>reading DAGs while jobs are running</p>
      - <p>performance differences between the different Spark APIs</p>
      - <p>packaging and deploying a Spark app</p>
      - <p>configuring Spark in 3 different ways</p>
      - <p>understanding the state of the art in Spark internals</p>
      - <p>leveraging Catalyst and Tungsten for massive perf</p>
    - <p>Understanding Spark Memory, Caching and Checkpointing</p>
      - <p>Tuning Spark executor memory zones</p>
      - <p>caching for speedy data reuse</p>
      - <p>making the right tradeoffs between speed, memory usage and fault tolerance</p>
      - <p>using checkpoints when jobs are failing or you can't afford a recomputation</p>
    - <p>Partitioning</p>
      - <p>leveraging repartitions</p>
      - <p>using coalesce to avoid shuffles</p>
      - <p>picking the right number of partitions at a shuffle to match cluster capability</p>
      - <p>using custom partitioners for custom jobs</p>
    - <p>Cluster tuning, fixing problems</p>
      - <p>allocating the right resources in a cluster</p>
      - <p>fixing data skews and straggling tasks with salting</p>
      - <p>fixing serialization problems</p>
      - <p>using the right serializers for free perf improvements</p>

    <p>**This course is for Scala and Spark programmers who need to improve the run time and memory footprint of their jobs. If you've never done Scala or Spark, this course is not for you.** I'll generally recommend that you take the Spark Optimization course first, but it's not a requirement.</p>

  </Fragment>
</CourseLayout>
